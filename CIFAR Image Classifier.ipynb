{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification experiment for CIFAR-10 dataset \n",
    "\n",
    "CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html) contains 60,000 32*32 color images in 10 classes, with 6000 images per class. \n",
    "\n",
    "Task is to build a multiclass image classifier using 2D Convolutional Neural Networks (2D CNNs) to achieve the best possible classification accuracy for the test set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required python packages\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchnet.meter.confusionmeter as cm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import utils\n",
    "\n",
    "import models.resnet as resnet\n",
    "import models.simple_net as simple_net\n",
    "import models.simple_net_bn as simple_net_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Assign CUDA device for computations:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation for training image set - with flip augmentation\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "#transformation for testing image set\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Define the mini-batch size\n",
    "batch_size = 128\n",
    "\n",
    "#datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "\n",
    "#dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size : 50000\n",
      "Training batches : 391\n",
      "Test set size : 10000\n",
      "Testing batches : 79\n"
     ]
    }
   ],
   "source": [
    "#Find the dataset sizes and number of batches\n",
    "\n",
    "#train set size\n",
    "train_set_size = len(trainset)\n",
    "\n",
    "#test set size\n",
    "test_set_size = len(testset)\n",
    "\n",
    "#Number of mini-batches in trainset /testset\n",
    "train_batches = len(trainloader)\n",
    "test_batches = len(testloader)\n",
    "\n",
    "print(\"Train set size : \" + str(train_set_size))\n",
    "print(\"Training batches : \" + str(train_batches))\n",
    "print(\"Test set size : \" + str(test_set_size))\n",
    "print(\"Testing batches : \" + str(test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_MultiProcessingDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# get some random training images\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(viewloader)\n\u001b[1;32m----> 6\u001b[0m images, labels \u001b[39m=\u001b[39m dataiter\u001b[39m.\u001b[39;49mnext()\n\u001b[0;32m      9\u001b[0m \u001b[39m# show images\u001b[39;00m\n\u001b[0;32m     10\u001b[0m utils\u001b[39m.\u001b[39mimshow(torchvision\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mmake_grid(images))\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_MultiProcessingDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "#Visualize 4 random training images\n",
    "viewloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(viewloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "\n",
    "# show images\n",
    "utils.imshow(torchvision.utils.make_grid(images))\n",
    "utils.imshowtransform(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = simple_net.SimpleNet()\n",
    "simplenet_bn = simple_net_bn.SimpleNet_bn()\n",
    "resnet34 = resnet.ResNet34()\n",
    "\n",
    "# pushing the network to the GPU\n",
    "net = simplenet_bn.to(device)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Define the optimizer and the learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists for generating graphs\n",
    "epochs = list()\n",
    "training_loss = list()\n",
    "\n",
    "# loop over the dataset multiple times\n",
    "for epoch in range(50):  \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print('[Epoch : %d] train_loss: %.3f' %\n",
    "          (epoch + 1, running_loss / train_batches))\n",
    "    epochs.append(epoch)\n",
    "    training_loss.append(running_loss/train_batches)\n",
    "    running_loss = 0.0\n",
    "    scheduler.step()\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the epoch vs. training loss\n",
    "plt.figure(1)\n",
    "plt.title(\"Training Loss - CIFAR-10 Classification\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs, training_loss, color='r', label=\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a trained ResNet34 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving trained model\n",
    "#torch.save(net.state_dict(), \"trained_resnet_model.pth\")\n",
    "\n",
    "#Loading the model\n",
    "model = resnet.ResNet34().to(device)\n",
    "model.load_state_dict(torch.load(\"trained_resnet_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the trained model with testing data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the class-wise accuracy\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the confusion matrix for testing data\n",
    "confusion_matrix = cm.ConfusionMeter(10)\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        confusion_matrix.add(predicted, labels)\n",
    "    #print(confusion_matrix.conf)\n",
    "\n",
    "#Confusion matrix as a heatmap\n",
    "con_m = confusion_matrix.conf\n",
    "df_con_m = pd.DataFrame(con_m, index= [i for i in classes], columns = [i for i in classes])\n",
    "sn.set(font_scale= 1.1)\n",
    "sn.heatmap(df_con_m, annot=True,fmt='g' ,  annot_kws={\"size\" : 10}, cbar = False, cmap=\"Blues\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a91703ec33666c511a7f349cc54a5ef58e304d36d1f347e987cc7aed6c3c74e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
